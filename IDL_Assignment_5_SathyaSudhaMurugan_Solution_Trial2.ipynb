{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IDL_Assignment_5_SathyaSudhaMurugan_Solution_Trial2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNMBfK/ipENquMoKAP5x6wh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SathyaSudha-96/Deep-Learning-2021-22/blob/main/IDL_Assignment_5_SathyaSudhaMurugan_Solution_Trial2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdykab1VT7lL"
      },
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import time\n",
        "from keras.utils.vis_utils import plot_model"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEtbDIlpUVzl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c377e52-1a67-4c66-9ec0-3473a7592c36"
      },
      "source": [
        "# remove infrequent words. you can play with this parameter as it will likely impact model quality\n",
        "num_words = 20000\n",
        "(train_sequences, train_labels), (test_sequences, test_labels) = tf.keras.datasets.imdb.load_data(num_words=num_words)\n",
        "train_labels = train_labels.reshape(-1)\n",
        "test_labels = test_labels.reshape(-1)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "17473536/17464789 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82ao-_NvUYvp",
        "outputId": "e109fce3-7a20-436f-f2cb-104d102f5704"
      },
      "source": [
        "train_sequences[:2]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]),\n",
              "       list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 10156, 4, 1153, 9, 194, 775, 7, 8255, 11596, 349, 2637, 148, 605, 15358, 8003, 15, 123, 125, 68, 2, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 2, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 2, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95])],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbkRKXF6VN1i",
        "outputId": "7775a6d4-3b69-4b42-b1c4-a8395903d5bd"
      },
      "source": [
        "# labels are simply binary: sentiment can be positive or negative\n",
        "train_labels[:2]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3juZYD9hT5a",
        "outputId": "a2c32483-2657-42f4-8e0a-8e650c489879"
      },
      "source": [
        "# solution is padding all sequences to the maximum length.\n",
        "# first find the maximum length\n",
        "sequence_lengths = [len(sequence) for sequence in train_sequences]\n",
        "max_len = max(sequence_lengths)\n",
        "total = sum(sequence_lengths)\n",
        "length = len(sequence_lengths)\n",
        "average = total/length\n",
        "average"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "238.71364"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "93a1S6YkhXG3",
        "outputId": "c4c7ccb1-020c-4abd-80b7-e5f509d4d7ae"
      },
      "source": [
        "# overview over sequence lengths in the data\n",
        "# could also look at mean, median, standard deviation...\n",
        "plt.hist(sequence_lengths, bins=80)\n",
        "plt.show()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIkGTkMihc_i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df373b22-b542-4031-92ff-aed943f7f561"
      },
      "source": [
        "# luckily there is a convenient function for padding\n",
        "train_sequences_padded = tf.keras.preprocessing.sequence.pad_sequences(train_sequences, maxlen=200, truncating='pre')\n",
        "train_sequences_padded.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 200)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qw38FSKvXpFq",
        "outputId": "57602967-88f0-4470-cd1f-870e653b27b2"
      },
      "source": [
        "# luckily there is a convenient function for padding\n",
        "test_sequences_padded = tf.keras.preprocessing.sequence.pad_sequences(test_sequences, maxlen=200, truncating='post')\n",
        "test_sequences_padded.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 200)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Fwsdl0MkO_N",
        "outputId": "26f8d446-acda-472d-810d-d4f387c40370"
      },
      "source": [
        "print('Train Labels Shape', train_labels.shape)\n",
        "print('Train Sequence Shape', train_sequences_padded.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Labels Shape (25000,)\n",
            "Train Sequence Shape (25000, 200)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLAb0jRPhgTo"
      },
      "source": [
        "# now we can create a dataset!\n",
        "train_data = tf.data.Dataset.from_tensor_slices((train_sequences_padded, train_labels.astype(np.int32)))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEHvUvSKXaVV"
      },
      "source": [
        "# now we can create a dataset!\n",
        "test_data = tf.data.Dataset.from_tensor_slices((test_sequences_padded, test_labels.astype(np.int32)))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hacCd7Lr7BrA"
      },
      "source": [
        "train_data = train_data.batch(128, drop_remainder=True)\n",
        "test_data = test_data.batch(128, drop_remainder=True)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1MVrU-B_xA-"
      },
      "source": [
        "hidden_dim = 200 # unfolded length of recurrent network\n",
        "output_dim = 1\n",
        "batch_size = 128\n",
        "dict_size = 20000"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIJQYj12_4bv"
      },
      "source": [
        "#weights from input to hidden\n",
        "w_ih = tf.Variable(np.zeros((dict_size,hidden_dim),dtype=np.float32))\n",
        "b = tf.Variable(np.zeros((hidden_dim,),dtype=np.float32))\n",
        "#weights from hidden to hidden\n",
        "w_hh = tf.Variable(np.zeros((hidden_dim, hidden_dim),dtype=np.float32))\n",
        " #weights from hidden to output\n",
        "w_ho = tf.Variable(np.zeros((hidden_dim,output_dim),dtype=np.float32))\n",
        "c = tf.Variable(np.zeros((output_dim,),dtype=np.float32))\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iYbtPSnTOeK"
      },
      "source": [
        "def RNN_model(sequence):\n",
        "  #old state\n",
        "  o_state = tf.Variable(np.zeros((batch_size,hidden_dim),dtype=np.float32))\n",
        "  for step in range(max_len):\n",
        "    new_state = seq[:,step]\n",
        "    x_t = tf.one_hot(new_state, depth=dict_size)\n",
        "    #a(t) = Wh(t-1) + UX(t)\n",
        "    a1 = tf.matmul(o_state,w_hh)\n",
        "    a2 = tf.matmul(x_t, w_ih)\n",
        "    a_t = tf.add(b,a1,a2) #a_t = b + tf.matmul(w_hh,o_state) + tf.matmul(w_ih,x_t)  \n",
        "    h_t = tf.math.tanh(a_t)   #h(t) = tanh(a(t))\n",
        "    o_state = h_t #assigning old state with new state\n",
        "    o_t = tf.add(c, tf.matmul(h_t,w_ho))\n",
        "    #output = tf.math.sigmoid(c + tf.matmul(w_ho,h_t))  #o(t) = Vh(t)\n",
        "    output = tf.math.sigmoid(o_t)\n",
        "    return output"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdw1DHvcFhsX",
        "outputId": "8cc87613-a323-458f-f882-824a1a0a4fdd"
      },
      "source": [
        "epochs = 30\n",
        "max_len = 200\n",
        "loss_function = tf.keras.losses.BinaryCrossentropy()\n",
        "accuracy = tf.keras.metrics.BinaryAccuracy()\n",
        "for epoch in range(epochs):\n",
        "  print('Starting Epoch',epoch+1)\n",
        "  for step, (sequence_batch, label_batch) in enumerate(train_data):\n",
        "    seq, lbl = sequence_batch, label_batch\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "      output = RNN_model(seq)\n",
        "      loss = loss_function(lbl,output)\n",
        "    #Compute gradients  \n",
        "    gradients = tape.gradient(loss,[w_ih, w_hh, w_ho, b, c])  \n",
        "    #Apply Gradients\n",
        "    w_ih.assign_sub = gradients[0]\n",
        "    w_hh.assign_sub = gradients[1]\n",
        "    w_ho.assign_sub = gradients[2]\n",
        "    b.assign_sub = gradients[3]\n",
        "    c.assign_sub = gradients[4]\n",
        "\n",
        "    accuracy(lbl,output)\n",
        "    \n",
        "    if not step % 100:\n",
        "      print(\"Loss: {} Training Accuracy: {}\".format(loss, accuracy.result()))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Epoch 1\n",
            "Loss: 0.6931469440460205 Training Accuracy: 0.578125\n",
            "Loss: 0.6931469440460205 Training Accuracy: 0.4953589141368866\n",
            "Starting Epoch 2\n",
            "Loss: 0.6931469440460205 Training Accuracy: 0.5005580186843872\n",
            "Loss: 0.6931469440460205 Training Accuracy: 0.49852195382118225\n",
            "Starting Epoch 3\n",
            "Loss: 0.6931469440460205 Training Accuracy: 0.5003596544265747\n",
            "Loss: 0.6931469440460205 Training Accuracy: 0.4991725981235504\n",
            "Starting Epoch 4\n",
            "Loss: 0.6931469440460205 Training Accuracy: 0.5002933144569397\n",
            "Loss: 0.6931469440460205 Training Accuracy: 0.4994533658027649\n",
            "Starting Epoch 5\n",
            "Loss: 0.6931469440460205 Training Accuracy: 0.500260055065155\n",
            "Loss: 0.6931469440460205 Training Accuracy: 0.4996098279953003\n",
            "Starting Epoch 6\n",
            "Loss: 0.6931469440460205 Training Accuracy: 0.5002401471138\n",
            "Loss: 0.6931469440460205 Training Accuracy: 0.4997095763683319\n",
            "Starting Epoch 7\n",
            "Loss: 0.6931469440460205 Training Accuracy: 0.5002268552780151\n",
            "Loss: 0.6931469440460205 Training Accuracy: 0.49977871775627136\n",
            "Starting Epoch 8\n",
            "Loss: 0.6931469440460205 Training Accuracy: 0.5002173185348511\n",
            "Loss: 0.6931469440460205 Training Accuracy: 0.4998294711112976\n",
            "Starting Epoch 9\n",
            "Loss: 0.6931469440460205 Training Accuracy: 0.5002102255821228\n",
            "Loss: 0.6931469440460205 Training Accuracy: 0.4998683035373688\n",
            "Starting Epoch 10\n",
            "Loss: 0.6931469440460205 Training Accuracy: 0.5002046823501587\n",
            "Loss: 0.6931469440460205 Training Accuracy: 0.4998989701271057\n",
            "Starting Epoch 11\n",
            "Loss: 0.6931469440460205 Training Accuracy: 0.5002002120018005\n",
            "Loss: 0.6931469440460205 Training Accuracy: 0.49992382526397705\n",
            "Starting Epoch 12\n",
            "Loss: 0.6931469440460205 Training Accuracy: 0.5001965761184692\n",
            "Loss: 0.6931469440460205 Training Accuracy: 0.4999443590641022\n",
            "Starting Epoch 13\n",
            "Loss: 0.6931469440460205 Training Accuracy: 0.5001935362815857\n",
            "Loss: 0.6931469440460205 Training Accuracy: 0.49996158480644226\n",
            "Starting Epoch 14\n",
            "Loss: 0.6931469440460205 Training Accuracy: 0.5001909732818604\n",
            "Loss: 0.6931469440460205 Training Accuracy: 0.4999762773513794\n",
            "Starting Epoch 15\n",
            "Loss: 0.6931469440460205 Training Accuracy: 0.5001888275146484\n",
            "Loss: 0.6931469440460205 Training Accuracy: 0.49998897314071655\n",
            "Starting Epoch 16\n",
            "Loss: 0.6931469440460205 Training Accuracy: 0.5001869201660156\n",
            "Loss: 0.6931469440460205 Training Accuracy: 0.5\n",
            "Starting Epoch 17\n",
            "Loss: 0.6931469440460205 Training Accuracy: 0.5001852512359619\n",
            "Loss: 0.6931469440460205 Training Accuracy: 0.5000097155570984\n",
            "Starting Epoch 18\n",
            "Loss: 0.6931469440460205 Training Accuracy: 0.5001837611198425\n",
            "Loss: 0.6931469440460205 Training Accuracy: 0.500018298625946\n",
            "Starting Epoch 19\n",
            "Loss: 0.6931469440460205 Training Accuracy: 0.5001824498176575\n",
            "Loss: 0.6931469440460205 Training Accuracy: 0.5000259876251221\n",
            "Starting Epoch 20\n",
            "Loss: 0.6931469440460205 Training Accuracy: 0.5001813173294067\n",
            "Loss: 0.6931469440460205 Training Accuracy: 0.5000328421592712\n",
            "Starting Epoch 21\n",
            "Loss: 0.6931469440460205 Training Accuracy: 0.5001802444458008\n",
            "Loss: 0.6931469440460205 Training Accuracy: 0.5000390410423279\n",
            "Starting Epoch 22\n",
            "Loss: 0.6931469440460205 Training Accuracy: 0.5001792907714844\n",
            "Loss: 0.6931469440460205 Training Accuracy: 0.5000447034835815\n",
            "Starting Epoch 23\n",
            "Loss: 0.6931469440460205 Training Accuracy: 0.5001783967018127\n",
            "Loss: 0.6931469440460205 Training Accuracy: 0.5000498294830322\n",
            "Starting Epoch 24\n",
            "Loss: 0.6931469440460205 Training Accuracy: 0.5001776218414307\n",
            "Loss: 0.6931469440460205 Training Accuracy: 0.5000545382499695\n",
            "Starting Epoch 25\n",
            "Loss: 0.6931469440460205 Training Accuracy: 0.5001769065856934\n",
            "Loss: 0.6931469440460205 Training Accuracy: 0.5000588297843933\n",
            "Starting Epoch 26\n",
            "Loss: 0.6931469440460205 Training Accuracy: 0.5001762509346008\n",
            "Loss: 0.6931469440460205 Training Accuracy: 0.5000628232955933\n",
            "Starting Epoch 27\n",
            "Loss: 0.6931469440460205 Training Accuracy: 0.5001756548881531\n",
            "Loss: 0.6931469440460205 Training Accuracy: 0.5000664591789246\n",
            "Starting Epoch 28\n",
            "Loss: 0.6931469440460205 Training Accuracy: 0.5001750588417053\n",
            "Loss: 0.6931469440460205 Training Accuracy: 0.5000698566436768\n",
            "Starting Epoch 29\n",
            "Loss: 0.6931469440460205 Training Accuracy: 0.5001745223999023\n",
            "Loss: 0.6931469440460205 Training Accuracy: 0.5000730752944946\n",
            "Starting Epoch 30\n",
            "Loss: 0.6931469440460205 Training Accuracy: 0.5001740455627441\n",
            "Loss: 0.6931469440460205 Training Accuracy: 0.5000759959220886\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3fZTIM0ZQae",
        "outputId": "d8175d77-364c-4b82-ddc1-d0de1121673d"
      },
      "source": [
        "for step, (sequence_batch, label_batch) in enumerate(test_data):\n",
        "  seq, lbl = sequence_batch, label_batch\n",
        "  output = RNN_model(seq)\n",
        "  loss = loss_function(lbl,output)\n",
        "  accuracy(lbl,output)\n",
        "print('Testing Accuracy:', format(accuracy.result()))    "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing Accuracy: 0.500158965587616\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Atqd89asFr48"
      },
      "source": [
        "# Food For Thought"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qVpoOlaFxn_"
      },
      "source": [
        "##### #1: \n",
        "Given : All sequences are padded to the length of the longest sequence in the dataset.\n",
        " Why is this wasteful? Can you think of a smarter padding scheme that is more efficient? Consider the fact that RNNs can work on arbitrary sequence lengths, and that training minibatches are pretty much independent of each other.\n",
        "\n",
        "> By padding all the sequences to the max length, we are in turn increasing the number of parameter, which makes the model more complex.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udlHCrZlGU3Q"
      },
      "source": [
        "##### #2: \n",
        "Between truncating long sequences and removing them, which option do you think is better? Why?\n",
        "\n",
        "\n",
        "> At times, Truncating can result in sequence with incorrect meaning. SO , it is better to remove the sentence completely instead of retaining some misleading imformation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_rlT9u6GnJQ"
      },
      "source": [
        "##### #3: \n",
        "Can you think of a way to avoid the one-hot vectors completely? Even if you cannot implement it, a conceptual idea is fine.\n",
        "\n",
        "\n",
        ">  map 1-hot categorical values to lower-dimensional vectors, where each 1-hot vector is re-represented as a multivariate Gaussian. The paper Deep Knowledge Tracing, which says this approach is motivated by the idea of compressed sensing:\n",
        "BARANIUK, R. Compressive sensing. IEEE signal processing magazine 24, 4 (2007).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goT8GRrQGudj"
      },
      "source": [
        "##### #4: \n",
        "For the output and loss, you actually have two options:\n",
        "\n",
        "*   have an output layer with 2 units, and use sparse categorical cross-entropy as before (i.e. softmax activation).\n",
        "*   have a single output unit and use binary cross-entropy (i.e. sigmoid activation).\n",
        "\n",
        "How can it be that we can choose how many outputs we have, i.e. how can both be correct? Are there differences between both choices as well as (dis)advantages relative to each other?\n",
        "\n",
        "\n",
        "> More or less both softmax and sigmoid are similar.\n",
        "Whereas, computing final output from 1 neurons is time and parameter effective when compared to 2 output neurons.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHwGorj_G1TG"
      },
      "source": [
        "##### #5: \n",
        "All sequences start with the same special “beginning of sequence” token (coded by index 1). Given this fact, is there a point in learning an initial state? Why (not)?\n",
        "\n",
        "\n",
        "> I think it is better to explicitly define the initial state. \n",
        "But still the previous state changes for every other current state when we traverse through the time. And, the old state will be updated for time step.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhPm7HULG6LR"
      },
      "source": [
        "#####  #6: \n",
        "pad_sequences allows for pre or post padding. Try both to see the difference. Which option do you think is better? Recall that we use the final time step output from our model\n",
        "\n",
        "> \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8_GUMDmG-05"
      },
      "source": [
        "##### #7: \n",
        "Can you think of a way to prevent the RNN from computing new states on padded time steps? One idea might be to “pass through” the previous state in case the current time step is padding. Note that, within a batch, some sequences might be padded for a given time step while others are not."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-3VW4FQHDFR"
      },
      "source": [
        "##### #8: \n",
        "What could be the advantage of using methods like the above? What are disadvantages? Can you think of other methods to incorporate the full output sequence instead of just the final step?"
      ]
    }
  ]
}